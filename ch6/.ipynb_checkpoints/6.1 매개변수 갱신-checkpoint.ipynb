{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBg_scOlcOVM"
   },
   "source": [
    "# 6.1.2 확률적 경사 하강법(SGD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1705385995513,
     "user": {
      "displayName": "이서호",
      "userId": "10919828918893273958"
     },
     "user_tz": -540
    },
    "id": "z9rI-3vhd2gy"
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "  def __init__(self, lr=0.01):\n",
    "    self.lr = lr\n",
    "\n",
    "  def update(self, params, grads):\n",
    "    for key in params.keys():\n",
    "      params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoCsglapft1T"
   },
   "source": [
    "각 가중치와 편향을 학습률과 기울기에 비례하여 업데이트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "error",
     "timestamp": 1705385995514,
     "user": {
      "displayName": "이서호",
      "userId": "10919828918893273958"
     },
     "user_tz": -540
    },
    "id": "dMs5d0UigdzQ",
    "outputId": "bfa0da8f-b1df-4edb-e02e-5cff4241b0e1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TwoLayerNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-18d7a55c96dc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TwoLayerNet' is not defined"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(...)\n",
    "optimizer = SGD()\n",
    "\n",
    "for i in range(10000):\n",
    "  ...\n",
    "  x_batch, t_batch = get_mini_batch(...) # 미니배치\n",
    "  grad = network.gradient(x_batch, t_batch)\n",
    "  params = network.params\n",
    "  optimizer.update(params, grads)\n",
    "  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiWWB_Qljo7X"
   },
   "source": [
    "optimizer는 '최적화를 행하는 자'라는 뜻의 단어이다. 이 코드에서는 SGD가 그 역할을 한다. 매개변수 갱신은 optimizer가 수행한다.\n",
    "이처럼 최적화를 담당하는 클래스를 분리해 구현 하면 기능을 모듈화 하기 쉽다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gb6Kc2GekLq1"
   },
   "source": [
    "# 6.1.3 SGD의 단점\n",
    "SGD는 단순하고 구현하기 쉽지만 문제에 따라서 비효율적일 때가 있다. 예를 들어 다음 함수의 최소값을 구하는 문제를 생각해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4X8LB_4oDHH"
   },
   "source": [
    "<p align=\"center\"><img src=\"imgs/5-7s.png\" width=600></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtqED46opf_9"
   },
   "source": [
    "이것의 최적화 갱신 경로를 구해보면 다음 그림과 같이 그려진다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zjtu__splBk"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-4MwVf9pqMn"
   },
   "source": [
    "SGD의 단점은 비등방성(방향에 따라 성질, 즉 여기서는 기울가 달라지는 함수)에서는 탐색 경로가 비효율적이라는 것이다. 그렇기 때문에 이를 개선해주는 모멘텀, AdaGrad, Adam 이라는 방법들을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQnuOCeRpsPJ"
   },
   "source": [
    "# 6.1.4 모멘텀\n",
    "모멘텀은 운동량을 뜻하고 수식으로는 다음과 같이 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzH2L6jjqG9u"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQhslK7gqsgB"
   },
   "source": [
    "SGD에서처럼 갱신할 가중치 매개변수, 손실함수, 학습률 등이 있. v라는 변수는 속도를 뜻한다.\n",
    "위 식은 기울기 방향으로 힘을 받아 물체가 가속된다는 물리법칙을 나타낸다.\n",
    "모멘텀을 소스코드로 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1705385995514,
     "user": {
      "displayName": "이서호",
      "userId": "10919828918893273958"
     },
     "user_tz": -540
    },
    "id": "9J71z8AnrJ9B"
   },
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "\n",
    "    \"\"\"모멘텀 SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QynuV_0jvNn5"
   },
   "source": [
    "* np.zeros_like(val): val과 동일한 shape와 dtype을 가지며, 모든 원소가 0으로 초기화된 배열을 생성한다.\n",
    "* 따라서, self.v[key]에는 모든 가중치와 편향에 대한 속도가 0으로 초기화된 배열이 들어가게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YrQMJGlvU93"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KY-V5bO5fco"
   },
   "source": [
    "SGD에 비해 지그재그 정도가 덜하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l64D1Qlv5mLD"
   },
   "source": [
    "# 6.1.5 AdaGrad\n",
    "신경망 학습에서는 학습률이 중요하다. 이 값이 너무 작으면 학습 시간이 너무 길어지고, 반대로 너무 크면 발산하여 학습이 제대로 이뤄지지 않는다. 이 학습률을 정하는 효과적 기술로 학습률 감소가 있다. 학습을 진행하면서 학습률을점차 줄여가는 방법인데 처음에는 크게 학습하다가 조금씩 작게 학습하는 방법이다. AdaGrad는 각각의 매개변수에 적응적으로 학습률을 조정하면서 학습을 진행한다. 수식으로 나타내면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voMWQqW76C0p"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7nyP3GY6unq"
   },
   "source": [
    "h는 기존 기울기 값을 제곱하여 계속 더해준다. 그리고 매개변수를 갱신할 때 1/sqrt(h)를 곱하여 학습률을 조정한다. AdaGrad는 과거의 기울기를 제곱하여 계속 더해나가기 때문에 학습을 진행할 수록 갱신 강도가 약해진다. 그래서 무한대로 학습을 해나가면 어느 순간 갱신량이 0이 되어 전혀 갱신이 되지 않다. 이 문제를 개선한 기법으로 RMSProp 기법이 있다. 이것은 과거의 모든 기울기를 균일하게 더해가는 것이 아니라, 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영한다. 지수이동평균이라고도 하며 과거 기울기의 반영 규모를 기하급수적으로 감소시킨다.\n",
    "AdaGrad를 소스코드로 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1705385995514,
     "user": {
      "displayName": "이서호",
      "userId": "10919828918893273958"
     },
     "user_tz": -540
    },
    "id": "lPVfWaGP6v4o"
   },
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kChqs69j9t0y"
   },
   "source": [
    "마지막에 1e-7이라는 작은 값을 더해서 0으로 나누는 상황을 막는다는 것이 중요한 포인트이다. AdaGrad에 의한 최적화 갱신 경로는 다음과 같아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FurXxpmj9xP7"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgbDbYY_9y7J"
   },
   "source": [
    "# 6.1.6 Adam\n",
    "위의 두 기법을 융합한 기법이 Adam 이다. 매개변수 공간을 효율적으로 탐색하면서 하이퍼파라미터의 편향 보정이 진행된다.\n",
    "\n",
    "Adam에 의한 최적화 갱신 경로는 다음과 같아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAxgw4Ru-khy"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH6Cmyw1_V9s"
   },
   "source": [
    "* Adam은 하이퍼 파라미터를 3개 설정한다. 학습률, 1차 모멘텀용 계수, 2차 모멘텀용 계수 인데, 논문에 따르면 기본 설정 값은 0.9 와 0.999 이며 이 값이면 많은 경우에 좋은 결과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8p7Igs2R_3be"
   },
   "source": [
    "# 6.1.8 MNIST 데이터셋으로 본 갱신 방법 비교\n",
    "위의 4 방법을 MNIST 데이터 셋을 통해 비교해보면 그래프결과는 다음과 같이 나오게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gyU78B-A6VJ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-u9JFQy1A67v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QtE17YrBD1s"
   },
   "source": [
    "하이퍼파라미터인 학습률과 신경망의 구조에 따라 결과가 달라지지만 일반적으로 SGD보다 다른 기법들이 빠르게 학습하고 최종 정확도도 높다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPKSHxCAIKoP9F+lTrtpulb",
   "mount_file_id": "1kYEB_KLn0yiqW2gYJfQGQnwO8P_1ynkJ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
