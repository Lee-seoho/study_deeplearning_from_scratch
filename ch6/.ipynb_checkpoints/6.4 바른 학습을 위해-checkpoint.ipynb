{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8c226f",
   "metadata": {},
   "source": [
    "# 6.4.1 오버피팅\n",
    "\n",
    "overfitting이란 신경망이 훈련 데이터에만 지나치게 적응되어 그 외의 데이터에는 제대로 대응하지 못하는 상태를 말한다. 기계학습은 범용 성능을 지향하기 때문에 이를 억제하는 것이 중요하다.\n",
    "\n",
    "Overfitting은 주로 2가지 경우에 일어난다.\n",
    "* 매개변수가 많고 표현력이 높은 모델\n",
    "* 훈련 데이터가 적을 경우\n",
    "\n",
    "코드를 통해 살펴보면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "096feaba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmnist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_mnist\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_layer_net\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiLayerNet\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGD\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# weight decay（가중치 감쇠） 설정 =======================\n",
    "#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
    "weight_decay_lambda = 0.1\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac20761",
   "metadata": {},
   "source": [
    "train_acc_list와 test_acc_list에는 epoch 단위(모든 훈련 데이터를 한 번씩 본 단위)의 정확도를 저장한다. 위 코드를 실행하면 결과는 다음과 같이 나오게 된다.\n",
    "\n",
    "<p align=\"center\"><img src=\"imgs/6-15.png\" width=600></p>\n",
    "\n",
    "훈련데이터와 시험데이터의 차이가 크게 벌어지게 되는데 이것은 훈련 데이터에만 적응해서이다. 훈련 때 사용하지 않은 범용 데이터(시험 데이터)에는 제대로 대응하지 못 하는 것을 이 그래프로 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326af5b5",
   "metadata": {},
   "source": [
    "# 6.4.2 가중치 감소\n",
    "overfitting문제를 억제하기 위해 가중치 감소라는 방법을 사용한다. 학습 과정에서 큰 가중치에 대해서는 큰 페널티를 부과하여 overfitting을 억제하는 방법이다. 가중치 감소는 모든 가중치 각각의 손실 함수에 1/2λW를 더한다. 따라서 가중치의 기울기를 구하는 계산에서는 그 동안의 오차역전파법에 따른 정규화 항을 미분한 λW를 더한다. 가중치 감소를 이용하여 훈련 데이터와 시험데이터의 정확도를 비교하면 다음과 같다.\n",
    "\n",
    "<p align=\"center\"><img src=\"imgs/6-16.png\" width=600></p>\n",
    "\n",
    "가중치 감소를 사용하지 않은 위 그림과 비교해 보면 그 차이가 줄었다. 오버피팅이 억제 되었다는 소리다. 훈련데이터에 대한 정확도도 100%에 도달하지 못한 점도 주목하자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d910cbc",
   "metadata": {},
   "source": [
    "# 6.4.3 드롭아웃\n",
    "신경망 모델이 복잡해지면 가중치 감소만으로는 대응하기 어렵기 때문에 드롭아웃 기법을 사용한다. 드롭아웃은 뉴런을 임의로 삭제하면서 학습하는 방법이다. 훈련 때 은닉층의 뉴런을 무작위로 골라 삭제하고 삭제한 뉴런은 신호를 전달하지 않게 된다. 훈련때는 데이터를 흘릴 때마다 삭제할 뉴런을 무작위로 선택하고 시험 때는 모든 뉴런에 신호를 전달한다. 단 시험 때는 각 뉴런의 출력에 훈련 때 삭제 안 한 비율을 곱하여 출력한다.\n",
    "\n",
    "<p align=\"center\"><img src=\"imgs/6-17.png\" width=600></p>\n",
    "\n",
    "드롭 아웃을 코드로 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d0583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "    self.dropout_ratio = dropout_ratio\n",
    "    self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e6baff7",
   "metadata": {},
   "source": [
    "순전파 때마다 self.mask에 삭제할 뉴런을 False로 표시한다. self.mask는 x와 형상이 같은 배열을 무작위로 생성하고, 그 값이 dropout_ratio보다 큰 원소만 True로 설정한다. 역전파 때의 동작은 ReLU와 같다. 순전파때 신호를 통과시키는 뉴런은 역전파 때도 신호를 그대로 통과시키고 순전파 때 통과시키지 않은 뉴런은 역전파 때도 신호를 차단한다. 이 효과를 MNIST 데이터셋으로 확인하기 위해 코드를 구현하고 실행하면 다음과 같다.\n",
    "\n",
    "<p align=\"center\"><img src=\"imgs/6-18.png\" width=600></p>\n",
    "\n",
    "드롭아웃을 적용하면 훈련 데이터와 시험 데이터에 대한 정확도 차이도 줄고 훈련 데이터에 대한 정확도가 100%에 도달하지도 않는다. 이처럼 드롭아웃을 이용하면 표현력을 높이면서도 overfitting을 억제할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
